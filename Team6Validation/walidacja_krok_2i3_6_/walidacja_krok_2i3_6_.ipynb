{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f51287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from random import sample \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Reshape\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "from utils import read_filenames, prepare_datasets\n",
    "from processing import find_bounding_boxes, extract_and_resize, process_image, load_image, convert_to_feature_vectors\n",
    "from data_generator import ImageDataGenerator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004882e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../archive/images_gz2/images'\n",
    "files_per_directory = 100000\n",
    "img_size = 128\n",
    "\n",
    "# Initialize an empty list to store the images\n",
    "images = []\n",
    "\n",
    "# Traverse the directory structure\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    \n",
    "    limit = files_per_directory\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            # Construct the full path to the image\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Open the image\n",
    "            img = Image.open(file_path)\n",
    "            \n",
    "            # Resize the image to 32x32 pixels\n",
    "            img = img.resize((img_size, img_size))\n",
    "            \n",
    "            # Convert the image to a numpy array and normalize the pixel values\n",
    "            img_array = np.array(img) / 255.0\n",
    "            \n",
    "            # Add the image array to the list\n",
    "            images.append(img_array)\n",
    "            \n",
    "            # Adding a limit to images per directory\n",
    "            limit -= 1\n",
    "            if limit == 0:\n",
    "                break\n",
    "\n",
    "# Convert the list of images to a numpy array\n",
    "galaxies = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(galaxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies[0z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f37d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_train, galaxies_val, galaxies_test = prepare_datasets(galaxies, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3575aaa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = galaxies[30]\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bb = find_bounding_boxes(galaxies[30], 30)\n",
    "plt.imshow(image_bb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_galaxy = galaxies[random.randint(0, len(galaxies_train) - 1)]\n",
    "\n",
    "image = cv.imread(random_galaxy)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('Before processing')\n",
    "plt.imshow(image)\n",
    "\n",
    "processed_image = process_image(random_galaxy, 20, (80, 80))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('After processing')\n",
    "plt.imshow(processed_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836df470",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = 80\n",
    "\n",
    "train_gen = ImageDataGenerator(galaxies_train, batch_size, img_size)\n",
    "val_gen = ImageDataGenerator(galaxies_val, batch_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Sequential([\n",
    "    Flatten(input_shape=[80, 80]),\n",
    "    Dense(1000, activation=\"relu\"),\n",
    "    Dense(800, activation=\"relu\"),\n",
    "    Dense(600, activation=\"relu\"),\n",
    "    Dense(400, activation=\"relu\"),\n",
    "    Dense(200, activation=\"relu\"),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\")\n",
    "])\n",
    "\n",
    "# Define the decoder\n",
    "decoder = Sequential([\n",
    "    Dense(100, input_shape=[50], activation=\"relu\"),\n",
    "    Dense(200, activation=\"relu\"),\n",
    "    Dense(400, activation=\"relu\"),\n",
    "    Dense(600, activation=\"relu\"),\n",
    "    Dense(800, activation=\"relu\"),\n",
    "    Dense(1000, activation=\"relu\"),\n",
    "    Dense(80*80, activation=\"sigmoid\"),\n",
    "    Reshape([80, 80])\n",
    "])\n",
    "\n",
    "autoencoder = Sequential([encoder, decoder])\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}/{10}\")\n",
    "\n",
    "    autoencoder.fit(\n",
    "        train_gen,\n",
    "        epochs=1,\n",
    "        validation_data=val_gen,\n",
    "    )\n",
    "\n",
    "    train_gen.on_epoch_end()\n",
    "    val_gen.on_epoch_end()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e196e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sample_val = []\n",
    "\n",
    "for galaxy in galaxies_val[:100]:\n",
    "    image = process_image(galaxy, 30, target_size=(80, 80))\n",
    "    processed_sample_val.append(image)\n",
    "\n",
    "processed_sample_val = np.array(processed_sample_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5273b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitions = autoencoder.predict(processed_sample_val[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, len(predicitions) - 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Before\")\n",
    "plt.imshow(processed_sample_val[index])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"After Auto Encoder\")\n",
    "plt.imshow(predicitions[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a44ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(include_top=False, input_shape=(80, 80, 3)) # VGG16 accepts only 3 inputs channels\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = convert_to_feature_vectors(galaxies_train, autoencoder, model, 10000)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30831014",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e43b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=700, random_state=21)\n",
    "pca.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b54606",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "xi = np.arange(0, 700, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 701, step=50)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 1, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec79500",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_reduced = pca.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_wcss_scores(X, k_max):\n",
    "    scores = []\n",
    "    for k in range(1, k_max+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(X)\n",
    "        wcss = kmeans.score(X) * -1 # score returns -WCSS\n",
    "        scores.append(wcss)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss_vec = count_wcss_scores(x, 10)\n",
    "x_ticks = list(range(1, len(wcss_vec) + 1))\n",
    "plt.plot(x_ticks, wcss_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Within-cluster sum of squares')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clustering_scores(X, cluster_num, model, score_fun):\n",
    "    if isinstance(cluster_num, int):\n",
    "        cluster_num_iter = [cluster_num]\n",
    "    else:\n",
    "        cluster_num_iter = cluster_num\n",
    "        \n",
    "    scores = []    \n",
    "    for k in cluster_num_iter:\n",
    "        model_instance = model(n_clusters=k)\n",
    "        labels = model_instance.fit_predict(X)\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)\n",
    "    \n",
    "    if isinstance(cluster_num, int):\n",
    "        return scores[0]\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa817191",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num_seq = range(2, 11) # Niektóre metryki nie działają gdy mamy tylko jeden klaster\n",
    "silhouette_vec = count_clustering_scores(x, cluster_num_seq, KMeans, silhouette_score)\n",
    "plt.plot(cluster_num_seq, silhouette_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60131f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {} # cluster_id : images\n",
    "for file, cluster in zip(galaxies_train[:len(x)], kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86834b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "fig, axs = plt.subplots(len(groups), n, figsize=(15, 15))\n",
    "\n",
    "for i, cluster in enumerate(groups.keys()):\n",
    "    for j in range(n):\n",
    "        filename = f'../data/images/{groups[cluster][j+5]}'\n",
    "        img = plt.imread(filename)\n",
    "        axs[i, j].imshow(img)\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].set_title(f\"Cluster {cluster}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = 80\n",
    "\n",
    "train_gen = ImageDataGenerator(galaxies_train, batch_size, img_size)\n",
    "val_gen = ImageDataGenerator(galaxies_val, batch_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Sequential([\n",
    "    Flatten(input_shape=[80, 80]),\n",
    "    Dense(1000, activation=\"relu\"),\n",
    "    Dense(800, activation=\"relu\"),\n",
    "    Dense(600, activation=\"relu\"),\n",
    "    Dense(400, activation=\"relu\"),\n",
    "    Dense(200, activation=\"relu\"),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\")\n",
    "])\n",
    "\n",
    "# Define the decoder\n",
    "decoder = Sequential([\n",
    "    Dense(100, input_shape=[50], activation=\"relu\"),\n",
    "    Dense(200, activation=\"relu\"),\n",
    "    Dense(400, activation=\"relu\"),\n",
    "    Dense(600, activation=\"relu\"),\n",
    "    Dense(800, activation=\"relu\"),\n",
    "    Dense(1000, activation=\"relu\"),\n",
    "    Dense(80*80, activation=\"sigmoid\"),\n",
    "    Reshape([80, 80])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Encoder\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')([80, 80])\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "encoded = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Decoder\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')([50])\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model([80, 80], decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}/{10}\")\n",
    "\n",
    "    autoencoder.fit(\n",
    "        train_gen,\n",
    "        epochs=1,\n",
    "        validation_data=val_gen,\n",
    "    )\n",
    "\n",
    "    train_gen.on_epoch_end()\n",
    "    val_gen.on_epoch_end()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd24625",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sample_val = []\n",
    "\n",
    "for galaxy in galaxies_val[:100]:\n",
    "    image = process_image(galaxy, 30, target_size=(80, 80))\n",
    "    processed_sample_val.append(image)\n",
    "\n",
    "processed_sample_val = np.array(processed_sample_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f205bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitions = autoencoder.predict(processed_sample_val[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, len(predicitions) - 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Before\")\n",
    "plt.imshow(processed_sample_val[index])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"After Auto Encoder\")\n",
    "plt.imshow(predicitions[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23639952",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(include_top=False, input_shape=(80, 80, 3)) # VGG16 accepts only 3 inputs channels\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868077de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = convert_to_feature_vectors(galaxies_train, autoencoder, model, 10000)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ebc9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=700, random_state=21)\n",
    "pca.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "xi = np.arange(0, 700, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 701, step=50)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 1, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d15df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_reduced = pca.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2160762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_wcss_scores(X, k_max):\n",
    "    scores = []\n",
    "    for k in range(1, k_max+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(X)\n",
    "        wcss = kmeans.score(X) * -1 # score returns -WCSS\n",
    "        scores.append(wcss)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdfa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss_vec = count_wcss_scores(x, 10)\n",
    "x_ticks = list(range(1, len(wcss_vec) + 1))\n",
    "plt.plot(x_ticks, wcss_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Within-cluster sum of squares')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clustering_scores(X, cluster_num, model, score_fun):\n",
    "    if isinstance(cluster_num, int):\n",
    "        cluster_num_iter = [cluster_num]\n",
    "    else:\n",
    "        cluster_num_iter = cluster_num\n",
    "        \n",
    "    scores = []    \n",
    "    for k in cluster_num_iter:\n",
    "        model_instance = model(n_clusters=k)\n",
    "        labels = model_instance.fit_predict(X)\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)\n",
    "    \n",
    "    if isinstance(cluster_num, int):\n",
    "        return scores[0]\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num_seq = range(2, 11) # Niektóre metryki nie działają gdy mamy tylko jeden klaster\n",
    "silhouette_vec = count_clustering_scores(x, cluster_num_seq, KMeans, silhouette_score)\n",
    "plt.plot(cluster_num_seq, silhouette_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fec4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {} # cluster_id : images\n",
    "for file, cluster in zip(galaxies_train[:len(x)], kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18315a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "fig, axs = plt.subplots(len(groups), n, figsize=(15, 15))\n",
    "\n",
    "for i, cluster in enumerate(groups.keys()):\n",
    "    for j in range(n):\n",
    "        filename = f'../data/images/{groups[cluster][j+5]}'\n",
    "        img = plt.imread(filename)\n",
    "        axs[i, j].imshow(img)\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].set_title(f\"Cluster {cluster}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = 80\n",
    "\n",
    "train_gen = ImageDataGenerator(galaxies_train, batch_size, img_size)\n",
    "val_gen = ImageDataGenerator(galaxies_val, batch_size, img_size)\n",
    "\n",
    "encoder = Sequential([\n",
    "    Flatten(input_shape=[80, 80]),\n",
    "    Dense(1000, activation=\"relu\"),\n",
    "    Dense(800, activation=\"relu\"),\n",
    "    Dense(600, activation=\"relu\"),\n",
    "    Dense(400, activation=\"relu\"),\n",
    "    Dense(200, activation=\"relu\"),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\")\n",
    "])\n",
    "\n",
    "# Define the decoder\n",
    "decoder = Sequential([\n",
    "    Dense(100, input_shape=[50], activation=\"relu\"),\n",
    "    Dense(200, activation=\"relu\"),\n",
    "    Dense(400, activation=\"relu\"),\n",
    "    Dense(600, activation=\"relu\"),\n",
    "    Dense(800, activation=\"relu\"),\n",
    "    Dense(1000, activation=\"relu\"),\n",
    "    Dense(80*80, activation=\"sigmoid\"),\n",
    "    Reshape([80, 80])\n",
    "])\n",
    "\n",
    "autoencoder = Sequential([encoder, decoder])\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}/{10}\")\n",
    "\n",
    "    autoencoder.fit(\n",
    "        train_gen,\n",
    "        epochs=1,\n",
    "        validation_data=val_gen,\n",
    "    )\n",
    "\n",
    "    train_gen.on_epoch_end()\n",
    "    val_gen.on_epoch_end()\n",
    "\n",
    "    print()\n",
    "\n",
    "processed_sample_val = []\n",
    "\n",
    "for galaxy in galaxies_val[:100]:\n",
    "    image = process_image(galaxy, 30, target_size=(80, 80))\n",
    "    processed_sample_val.append(image)\n",
    "\n",
    "processed_sample_val = np.array(processed_sample_val)\n",
    "\n",
    "predicitions = autoencoder.predict(processed_sample_val[:100])\n",
    "\n",
    "index = random.randint(0, len(predicitions) - 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Before\")\n",
    "plt.imshow(processed_sample_val[index])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"After Auto Encoder\")\n",
    "plt.imshow(predicitions[index])\n",
    "plt.show()\n",
    "\n",
    "model = VGG16(include_top=False, input_shape=(80, 80, 3)) # VGG16 accepts only 3 inputs channels\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "\n",
    "features = convert_to_feature_vectors(galaxies_train, autoencoder, model, 10000)\n",
    "features.shape\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "pca = PCA(n_components=700, random_state=21)\n",
    "pca.fit(features)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "xi = np.arange(0, 700, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 701, step=50)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 1, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95)\n",
    "\n",
    "features_reduced = pca.transform(features)\n",
    "\n",
    "def count_wcss_scores(X, k_max):\n",
    "    scores = []\n",
    "    for k in range(1, k_max+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(X)\n",
    "        wcss = kmeans.score(X) * -1 # score returns -WCSS\n",
    "        scores.append(wcss)\n",
    "    return scores\n",
    "\n",
    "wcss_vec = count_wcss_scores(x, 10)\n",
    "x_ticks = list(range(1, len(wcss_vec) + 1))\n",
    "plt.plot(x_ticks, wcss_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Within-cluster sum of squares')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "def count_clustering_scores(X, cluster_num, model, score_fun):\n",
    "    if isinstance(cluster_num, int):\n",
    "        cluster_num_iter = [cluster_num]\n",
    "    else:\n",
    "        cluster_num_iter = cluster_num\n",
    "        \n",
    "    scores = []    \n",
    "    for k in cluster_num_iter:\n",
    "        model_instance = model(n_clusters=k)\n",
    "        labels = model_instance.fit_predict(X)\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)\n",
    "    \n",
    "    if isinstance(cluster_num, int):\n",
    "        return scores[0]\n",
    "    else:\n",
    "        return scores\n",
    "\n",
    "cluster_num_seq = range(2, 11) # Niektóre metryki nie działają gdy mamy tylko jeden klaster\n",
    "silhouette_vec = count_clustering_scores(x, cluster_num_seq, KMeans, silhouette_score)\n",
    "plt.plot(cluster_num_seq, silhouette_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(x)\n",
    "\n",
    "groups = {} # cluster_id : images\n",
    "for file, cluster in zip(galaxies_train[:len(x)], kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)\n",
    "\n",
    "n = 5\n",
    "fig, axs = plt.subplots(len(groups), n, figsize=(15, 15))\n",
    "\n",
    "for i, cluster in enumerate(groups.keys()):\n",
    "    for j in range(n):\n",
    "        filename = f'../data/images/{groups[cluster][j+5]}'\n",
    "        img = plt.imread(filename)\n",
    "        axs[i, j].imshow(img)\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].set_title(f\"Cluster {cluster}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
